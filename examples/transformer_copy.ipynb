{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this to allow for local imports.\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from required modules.\n",
    "from tommy2tommy.models.transformer import TransformerLM\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the configuration hyperparameters.\n",
    "config = {\n",
    "    # Model/data hyperparameters.\n",
    "    'vocab_size': 32,\n",
    "    'length': 10,\n",
    "    'num_layers': 2,\n",
    "    'd_model': 32,\n",
    "    'd_filter': 128,\n",
    "    'num_heads': 8,\n",
    "    'dropout_rate': 0.1,\n",
    "    'ffn_activation': 'gelu',\n",
    "    'layer_norm_epsilon': 1.0e-6,\n",
    "    \n",
    "    # Optimizer hyperparameters.\n",
    "    'adam_learning_rate': 0.001,\n",
    "    'adam_beta_1': 0.9,\n",
    "    'adam_beta_2': 0.999,\n",
    "    'adam_epsilon': 1.0e-7,\n",
    "    \n",
    "    # Training hyperparameters.\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'training_steps': 1000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs, create the synthetic datasets.\n",
    "def generate_input(vocab_size, length):\n",
    "    assert length % 2 == 0\n",
    "    half_len = (length - 2)//2\n",
    "    while True:\n",
    "        half_input = tf.random.uniform(shape=(half_len,), minval=1, maxval=vocab_size, dtype=tf.int32)\n",
    "        full_input = tf.concat([[0], half_input, [0], half_input], axis=0)\n",
    "        yield (full_input, full_input)\n",
    "\n",
    "# Need to specify the output shapes.\n",
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: generate_input(config['vocab_size'], config['length']),\n",
    "    output_types=(tf.int32, (tf.int32)),\n",
    "    output_shapes=((config['length'],), (config['length'],)))\n",
    "\n",
    "# Batch the training data, must drop the remainder in order for the input sizes to be consistent.\n",
    "training_dataset = training_dataset.batch(config['batch_size'], drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the loss function, should only calculate loss on the copied half of outputs.\n",
    "def loss_function(real, pred):\n",
    "    real = real[:, config['length']//2:]\n",
    "    pred = pred[:, config['length']//2:, :]\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(real, pred, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# Same as above for accuracy.\n",
    "def accuracy(real, pred):\n",
    "    real = real[:, config['length']//2:]\n",
    "    pred = pred[:, config['length']//2:, :]\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(real, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam optimizer. Works best with learning rate warmup, but this task is easy enough it's not necessary.\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    config['adam_learning_rate'],\n",
    "    beta_1=config['adam_beta_1'],\n",
    "    beta_2=config['adam_beta_2'],\n",
    "    epsilon=config['adam_epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the language model and compile.\n",
    "model = TransformerLM(config, padding_id=-1)  # No padding in our synthetic data.\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PaddingBias.call of <tommy2tommy.layers.bias.PaddingBias object at 0x00000203EA1DF070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PaddingBias.call of <tommy2tommy.layers.bias.PaddingBias object at 0x00000203EA1DF070>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method CausalBias.call of <tommy2tommy.layers.bias.CausalBias object at 0x00000203F11CBC40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method CausalBias.call of <tommy2tommy.layers.bias.CausalBias object at 0x00000203F11CBC40>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method ShiftRight.call of <tommy2tommy.layers.attention.ShiftRight object at 0x00000203F11E80D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method ShiftRight.call of <tommy2tommy.layers.attention.ShiftRight object at 0x00000203F11E80D0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method PositionalEncoding.call of <tommy2tommy.layers.embeddings.PositionalEncoding object at 0x00000203F11E8790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method PositionalEncoding.call of <tommy2tommy.layers.embeddings.PositionalEncoding object at 0x00000203F11E8790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SplitHeads.call of <tommy2tommy.layers.attention.SplitHeads object at 0x00000203F152D430>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SplitHeads.call of <tommy2tommy.layers.attention.SplitHeads object at 0x00000203F152D430>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DotProductAttention.call of <tommy2tommy.layers.attention.DotProductAttention object at 0x00000203F152D1F0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DotProductAttention.call of <tommy2tommy.layers.attention.DotProductAttention object at 0x00000203F152D1F0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MergeHeads.call of <tommy2tommy.layers.attention.MergeHeads object at 0x00000203F152D5E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MergeHeads.call of <tommy2tommy.layers.attention.MergeHeads object at 0x00000203F152D5E0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function loss_function at 0x00000203F11E3430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function loss_function at 0x00000203F11E3430> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function accuracy at 0x00000203F11E34C0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function accuracy at 0x00000203F11E34C0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 10s 10ms/step - loss: 2.7902 - accuracy: 0.2255\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 2.1258 - accuracy: 0.3816\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.6787 - accuracy: 0.8085\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.4171 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.3382 - accuracy: 0.9061\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2881 - accuracy: 0.9202\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2466 - accuracy: 0.9309\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.2282 - accuracy: 0.9370\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.2070 - accuracy: 0.9437\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1821 - accuracy: 0.9497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x203f157c340>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model. Doesn't really make sense to validate since the input is randomly generated.\n",
    "model.fit(training_dataset,\n",
    "          epochs=config['num_epochs'],\n",
    "          steps_per_epoch=config['training_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Example inference. Note the extra padding token from the rightward shift in the language model.\n",
    "# Note also that the model only learns the second half, due to our choice of loss function.\n",
    "example = tf.constant([[0, 0, 1, 2, 3, 4, 0, 0, 0, 0]])\n",
    "print(tf.argmax(model.predict(x=example), axis=2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The correct way to do inference is with a decoder search algorithm such as greedy search or beam search.\n",
    "from tommy2tommy.utils.search import greedy_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0 30  1  2 11  0 30  1  2 11]]\n"
     ]
    }
   ],
   "source": [
    "example = tf.constant([[0, 30, 1, 2, 11, 0]])\n",
    "print(greedy_search(model, prefix=example, length=config['length']).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
